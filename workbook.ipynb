{"cells":[{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["import io, os, sys, json, pprint, requests, base64\n","from PIL import Image, ImageDraw\n","from IPython import display\n","\n","# Rerun to reset filename and present dialog\n","image_file_name = ''\n","def select_image_file():\n","    from tkinter import Tk     # from tkinter import Tk for Python 3.x\n","    from tkinter.filedialog import askopenfilename\n","\n","    Tk().withdraw() # we don't want a full GUI, so keep the root window from appearing\n","    filename = askopenfilename(filetypes=((\"Jpeg\", \"*.jpg\"),)) # show an \"Open\" dialog box and return the path to the selected file\n","    return filename or sys.exit(1)\n","\n","# Damnit, FIX:\n","class glob: pass\n","db = glob()\n","\n","# Also load azure cognitive services endpoint uri and our keys. as store in the same file... \n","with open(\"photo-manager-proto.json\") as f:\n","    db.creds = json.load(f)\n","\n","# from ImageDraw.ImageColor.colormap, to draw with...\n","colors = ['red', 'green', 'blue', 'yellow', 'orange', 'peachpuff', 'purple']\n","\n","if not image_file_name:\n","    image_file_name = select_image_file()\n","\n","# Let's read some image data into a variable that can be used by succeding methods\n","with open(image_file_name, \"rb\") as image_file:\n","    db.image_data = image_file.read()\n","\n","# For making REST calls to azure cognitive services\n","# We pass binary image data in body...\n","def get_cognitive_data(url, params, body):\n","    with requests.post(\"{0}{1}\".format(db.creds['cognitive_services_uri'], url), data=body, params=params, \n","            headers={\n","                'Content-Type': 'application/octet-stream', \n","                \"Ocp-Apim-Subscription-Key\": db.creds[\"cognitive_services_key1\"]\n","            }) as request:\n","        return request.json() "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"=== Just display a scaled version of the image...\")\n","pillow_image = Image.open(io.BytesIO(db.image_data)).convert(\"RGBA\")\n","pillow_image.thumbnail((512, 512))\n","display.display(pillow_image)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Bashing my head agains the wall... again... and again... the python client lib code is a tad hard to navigate and grok...\n","# so had to give up trying to retreive a bearer token from it... perchance query param api-key would work\n","print(\"=== Google Cloud Vision REST annotate endpoint:\")\n","body = {\n","    \"requests\": [\n","        {\n","            \"image\": { \"content\": base64.b64encode(db.image_data).decode() }, \n","            \"features\": [\n","                # {\"type\": \"LABEL_DETECTION\"},\n","                {\"type\": \"FACE_DETECTION\", \"model\": \"builtin/latest\"},\n","                # {\"type\": \"OBJECT_LOCALIZATION\"},\n","                # {\"type\": \"TEXT_DETECTION\", \"model\": \"builtin/latest\"},\n","                {\"type\": \"TYPE_UNSPECIFIED\", \"model\": \"builtin/latest\"},\n","                {\"type\": \"LANDMARK_DETECTION\", \"model\": \"builtin/latest\"},\n","                {\"type\": \"LOGO_DETECTION\", \"model\": \"builtin/latest\"},\n","                {\"type\": \"SAFE_SEARCH_DETECTION\", \"model\": \"builtin/latest\"},\n","                # {\"type\": \"IMAGE_PROPERTIES\"},\n","                # {\"type\": \"CROP_HINTS\"},\n","                {\"type\": \"WEB_DETECTION\", \"model\": \"builtin/latest\"},\n","                {\"type\": \"PRODUCT_SEARCH\", \"model\": \"builtin/latest\"},\n","            ],\n","        }\n","    ]\n","}\n","\n","params = { \n","    # \"fields\": \"responses(faceAnnotations,safeSearchAnnotation,webDetection(bestGuessLabels,webEntities))\",\n","    \"key\": db.creds[\"vision_api_key\"]\n","}\n","with requests.post(db.creds[\"vision_annotate_url\"], params=params, json=body) as r:\n","    responses = r.json()\n","    for response in responses['responses']:\n","        for annotation in response['faceAnnotations']:\n","            del annotation['landmarks']\n","\n","pillow_image = Image.open(io.BytesIO(db.image_data)).convert(\"RGBA\")\n","image_draw = ImageDraw.Draw(pillow_image)\n","for i, face in enumerate(responses['responses'][0]['faceAnnotations']):\n","    vertices = face['boundingPoly']['vertices']\n","    image_draw.rectangle(xy=(vertices[0]['x'], vertices[0]['y'], vertices[2]['x'], vertices[2]['y']),\n","            outline=colors[i%len(colors)], width=5)\n","\n","pprint.pprint(responses, compact=True, width=120)\n","pillow_image.thumbnail((1024, 1024))\n","display.display(pillow_image)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"=== Azure Cognitive Services - Face API: Detect\")\n","\n","params = {\n","    # Request parameters\n","    'returnFaceId': 'true',\n","    'returnFaceLandmarks': 'true',\n","    'returnFaceAttributes': 'accessories,age,blur,emotion,exposure,facialhair,gender,glasses,hair,headpose,makeup,noise,occlusion,qualityforrecognition,smile',\n","    'recognitionModel': 'recognition_04',\n","    'returnRecognitionModel': 'true',\n","    'detectionModel': 'detection_01',\n","    'faceIdTimeToLive': '86400',\n","}\n","\n","faces = get_cognitive_data(\"/face/v1.0/detect\", params, db.image_data)\n","# faceLandmarks pollutes the output of pprint, let's drep em'\n","# for face in faces:\n","#     del face['faceLandmarks']\n","# pprint.pprint(faces)\n","\n","pillow_image = Image.open(io.BytesIO(db.image_data)).convert(\"RGBA\")\n","image_draw = ImageDraw.Draw(pillow_image)\n","\n","for i, face in enumerate(faces):\n","    face_rectangle = face[\"faceRectangle\"]\n","    text = f\"\\nFace {i}\\n\\tLocation: {face_rectangle}\\n\\tAge: {face['faceAttributes']['age']}\\n\\tGlasses: {face['faceAttributes']['glasses']}\\n\\t\"\n","    text += f\"Gender: {face['faceAttributes']['gender']}\\n\\tEmotions: {face['faceAttributes']['emotion']}\"\n","    print(text)\n","    image_draw.rectangle((face_rectangle['left'], face_rectangle['top'], \n","            face_rectangle['left']+face_rectangle['width'], face_rectangle['top']+face_rectangle['height']), outline=colors[i%len(colors)], width=5)\n","    # image_draw.multiline_text((face_rectangle['left'], face_rectangle['top']), text, font=ImageFont.truetype(\"tahoma.ttf\", size=48))\n","\n","pillow_image.thumbnail((1024, 1024))\n","display.display(pillow_image)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"=== Azure Cognitive Services - Vision API: Describe\")\n","\n","params = {\n","    'maxCandidates': '3',\n","    'language': 'en',\n","    'model-version': 'latest',\n","}\n","\n","data = get_cognitive_data(\"/vision/v3.2/describe\", params, db.image_data)\n","pprint.pprint(data)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"=== Azsure Cognitive Services - Vision API: Analyze\")\n","\n","params = {\n","    \"visualFeatures\": \"Categories,Adult,Tags,Brands,Color,Description,Faces,ImageType,Objects\",\n","    \"details\": \"Celebrities,Landmarks\",\n","    \"language\": \"en\",\n","    \"model-version\": \"latest\"\n","}\n","\n","data = get_cognitive_data(\"/vision/v3.2/analyze\", params, db.image_data)\n","pprint.pprint(data)"]}],"metadata":{"interpreter":{"hash":"582074b5810dd446ddde912c2f1544cdd98531d3e85332d22fc77fc20827ef0f"},"kernelspec":{"display_name":"Python 3.10.2 64-bit ('venv10': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.2"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
