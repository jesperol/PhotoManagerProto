{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Cloud Vision\n",
    "# Initialize ImageAnnotatorClient\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import io, os, json\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"photo-manager-proto.json\" \n",
    "\n",
    "with open(\"photo-manager-proto.json\") as f:\n",
    "    db = json.load(f)\n",
    "\n",
    "from google.cloud import vision\n",
    "\n",
    "client = vision.ImageAnnotatorClient()\n",
    "\n",
    "# from ImageDraw.ImageColor.colormap, to draw with...\n",
    "colors = ['red', 'green', 'blue', 'yellow', 'orange', 'peachpuff', 'purple']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.abspath('marstrand.jpg')\n",
    "with open(file_path, \"rb\") as image_file:\n",
    "    image_data = image_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Cloud Vision...\")\n",
    "vision_image = vision.Image(content=image_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Labels:\")\n",
    "response = client.label_detection(image=vision_image)\n",
    "print(response)\n",
    "#for label in label_results.label_annotations:\n",
    "#    print(f'{label.description} - {label.score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=== Safe search:')\n",
    "response = client.safe_search_detection(image=vision_image)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=== Landmarks:')\n",
    "response = client.landmark_detection(image=vision_image)\n",
    "print(response)\n",
    "landmarks = response.landmark_annotations\n",
    "\n",
    "for landmark in landmarks:\n",
    "    print(landmark.description)\n",
    "    for location in landmark.locations:\n",
    "        lat_lng = location.lat_lng\n",
    "        print('Latitude {}'.format(lat_lng.latitude))\n",
    "        print('Longitude {}'.format(lat_lng.longitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=== Faces:')\n",
    "response = client.face_detection(image=vision_image)\n",
    "\n",
    "pillow_image = Image.open(io.BytesIO(image_data)).convert(\"RGBA\")\n",
    "image_draw = ImageDraw.Draw(pillow_image)\n",
    "for i, face in enumerate(response.face_annotations):\n",
    "    vertices = (['({},{})'.format(vertex.x, vertex.y) for vertex in face.bounding_poly.vertices]) \n",
    "    print(f'Face #{i}, bounds: {\", \".join(vertices)}')\n",
    "    print(face)\n",
    "    xs = list(map(lambda v: v.x, face.bounding_poly.vertices))\n",
    "    ys = list(map(lambda v: v.y, face.bounding_poly.vertices))\n",
    "    image_draw.rectangle((min(xs), min(ys), max(xs), max(ys)), outline=\"blue\", width=3)\n",
    "        \n",
    "from IPython import display\n",
    "# pillow_image.thumbnail((512,512))\n",
    "# display.display(pillow_image)\n",
    "\n",
    "# for mark in face.landmarks:\n",
    "#     print(f'{mark.type_}({mark.position.x},{mark.position.y},{mark.position.z})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image():\n",
    "    for i, face in enumerate(response.face_annotations):\n",
    "        vertices = (['({},{})'.format(vertex.x, vertex.y) for vertex in face.bounding_poly.vertices]) \n",
    "        print(f'Face #{i}, bounds: {\", \".join(vertices)}')\n",
    "        print(face)\n",
    "        xs = list(map(lambda v: v.x, face.bounding_poly.vertices))\n",
    "        ys = list(map(lambda v: v.y, face.bounding_poly.vertices))\n",
    "        image_draw.rectangle((min(xs), min(ys), max(xs), max(ys)), outline=\"blue\", width=3)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def face_detect():\n",
    "    print(\"=== Face API: Detect\")\n",
    "\n",
    "    import http.client, urllib.request, urllib.parse, urllib.error, json \n",
    "\n",
    "    headers = {\n",
    "        # Request headers\n",
    "        # 'Content-Type': 'application/json',\n",
    "        'Content-Type': 'application/octet-stream',\n",
    "        \"Ocp-Apim-Subscription-Key\": db[\"key1\"],\n",
    "    }\n",
    "\n",
    "    params = urllib.parse.urlencode({\n",
    "        # Request parameters\n",
    "        'returnFaceId': 'true',\n",
    "        'returnFaceLandmarks': 'true',\n",
    "        'returnFaceAttributes': 'age,smile,facialHair,glasses,emotion,hair,makeup,accessories',\n",
    "        'recognitionModel': 'recognition_04',\n",
    "        'returnRecognitionModel': 'true',\n",
    "        'detectionModel': 'detection_01',\n",
    "        'faceIdTimeToLive': '86400',\n",
    "    })\n",
    "\n",
    "    endpoint = db['uri']\n",
    "    conn = http.client.HTTPSConnection(endpoint)\n",
    "    conn.request(\"POST\", \"/face/v1.0/detect?%s\" % params, body=image_data, headers=headers)\n",
    "    response = conn.getresponse()\n",
    "    data = response.read()\n",
    "    conn.close()\n",
    "\n",
    "    pillow_image = Image.open(io.BytesIO(image_data)).convert(\"RGBA\")\n",
    "    image_draw = ImageDraw.Draw(pillow_image)\n",
    "\n",
    "    for i, face in enumerate(json.loads(data)):\n",
    "        face_rectangle = face[\"faceRectangle\"]\n",
    "        text = f'\\nFace {i}\\n\\tLocation: {face_rectangle}\\n\\tAge: {face[\"faceAttributes\"][\"age\"]}\\n\\tEmotions: {face[\"faceAttributes\"][\"emotion\"]}'\n",
    "        print(text)\n",
    "        image_draw.rectangle((face_rectangle['left'], face_rectangle['top'], \n",
    "                face_rectangle['left']+face_rectangle['width'], face_rectangle['top']+face_rectangle['height']), outline=colors[i%len(colors)], width=3)\n",
    "        # image_draw.multiline_text((face_rectangle['left'], face_rectangle['top']), text, font=ImageFont.truetype(\"tahoma.ttf\", size=48))\n",
    "\n",
    "    from IPython import display\n",
    "    # pillow_image.thumbnail((1024, 1024))\n",
    "    display.display(pillow_image)\n",
    "\n",
    "face_detect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vision_detect():\n",
    "    print(\"=== Vision API: Describe\")\n",
    "    import http.client, urllib.request, urllib.parse, urllib.error, pprint\n",
    "\n",
    "    headers = {\n",
    "        # Request headers\n",
    "        'Content-Type': 'application/octet-stream',\n",
    "        \"Ocp-Apim-Subscription-Key\": db[\"key1\"],\n",
    "    }\n",
    "\n",
    "    params = urllib.parse.urlencode({\n",
    "        # Request parameters\n",
    "        'maxCandidates': '3',\n",
    "        'language': 'en',\n",
    "        'model-version': 'latest',\n",
    "    })\n",
    "\n",
    "    endpoint = db['uri']\n",
    "    conn = http.client.HTTPSConnection(endpoint)\n",
    "    conn.request(\"POST\", \"/vision/v3.2/describe?%s\" % params, body=image_data, headers=headers)\n",
    "    response = conn.getresponse()\n",
    "    data = response.read()\n",
    "    pprint.pprint(json.loads(data))\n",
    "    conn.close()\n",
    "\n",
    "vision_detect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vision_analyze():\n",
    "    print(\"=== Vision API: Analyze\")\n",
    "    import http.client, urllib.request, urllib.parse, urllib.error, base64, pprint\n",
    "\n",
    "    headers = {\n",
    "        # Request headers\n",
    "        'Content-Type': 'application/octet-stream',\n",
    "        \"Ocp-Apim-Subscription-Key\": db[\"key1\"],\n",
    "    }\n",
    "\n",
    "    params = urllib.parse.urlencode({\n",
    "        # Request parameters\n",
    "        'visualFeatures': 'Categories,Adult,Tags,Brands,Color,Description,Faces,ImageType,Objects',\n",
    "        'details': 'Celebrities,Landmarks',\n",
    "        'language': 'en',\n",
    "        'model-version': 'latest',\n",
    "    })\n",
    "    \n",
    "    endpoint = db['uri']\n",
    "    conn = http.client.HTTPSConnection(endpoint)\n",
    "    conn.request(\"POST\", \"/vision/v3.2/analyze?%s\" % params, body=image_data, headers=headers)\n",
    "    response = conn.getresponse()\n",
    "    data = response.read()   \n",
    "    pprint.pprint(json.loads(data))\n",
    "    conn.close()\n",
    "\n",
    "vision_analyze()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9dc8ab893de1eeb12238bd5f8a3522e0060b35d78747c82dd5784a806ff3da8c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('venvw': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
