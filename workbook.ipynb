{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, os, json, pprint, requests, base64\n",
    "from PIL import Image, ImageDraw\n",
    "from google.cloud import vision\n",
    "from IPython import display\n",
    "\n",
    "# Rerun to reset filename and present dialog\n",
    "image_file_name = ''\n",
    "def select_image_file():\n",
    "    from tkinter import Tk     # from tkinter import Tk for Python 3.x\n",
    "    from tkinter.filedialog import askopenfilename\n",
    "\n",
    "    Tk().withdraw() # we don't want a full GUI, so keep the root window from appearing\n",
    "    filename = askopenfilename(filetypes=((\"Jpeg\", \"*.jpg\"),)) # show an \"Open\" dialog box and return the path to the selected file\n",
    "    return  filename or \"marstrand.jpg\"\n",
    "\n",
    "# Damnit, FIX:\n",
    "class glob: pass\n",
    "db = glob()\n",
    "\n",
    "# Also load azure cognitive services endpoint uri and our keys. as store in the same file... \n",
    "with open(\"photo-manager-proto.json\") as f:\n",
    "    db.creds = json.load(f)\n",
    "\n",
    "# from ImageDraw.ImageColor.colormap, to draw with...\n",
    "colors = ['red', 'green', 'blue', 'yellow', 'orange', 'peachpuff', 'purple']\n",
    "\n",
    "if not image_file_name:\n",
    "    image_file_name = select_image_file()\n",
    "\n",
    "# Let's read some image data into a variable that can be used by succeding methods\n",
    "with open(image_file_name, \"rb\") as image_file:\n",
    "    db.image_data = image_file.read()\n",
    "\n",
    "# Google Cloud Vision client seem to like the image data wrapped like this\n",
    "# Needs a service account or something..\n",
    "# Set this environment variable so tat google.cloud libs knows where to get credentials\n",
    "# os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"photo-manager-proto.json\" \n",
    "# db.vision_image = vision.Image(content=db.image_data)\n",
    "# db.vision_client = vision.ImageAnnotatorClient()\n",
    "\n",
    "# For making REST calls to azure cognitive services\n",
    "# We pass binary image data in body...\n",
    "def get_cognitive_data(url, params, body):\n",
    "    with requests.post(f\"{db.creds['cognitive_services_uri']}{url}\", data=body, params=params, \n",
    "            headers={\n",
    "                'Content-Type': 'application/octet-stream', \n",
    "                \"Ocp-Apim-Subscription-Key\": db.creds[\"cognitive_services_key1\"]\n",
    "            }) as request:\n",
    "        return request.json() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pillow_image = Image.open(io.BytesIO(db.image_data)).convert(\"RGBA\")\n",
    "pillow_image.thumbnail((512, 512))\n",
    "display.display(pillow_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bashing my head agains the wall... again... and again... the python client lib code is a tad hard to navigate and grok...\n",
    "# so had to give up trying to retreive a bearer token from it... perchance query param api-key would work\n",
    "def ohmy():\n",
    "    print(\"=== Google Cloud Vision REST annotate endpoint:\")\n",
    "    body = {\n",
    "        \"requests\": [\n",
    "            {\n",
    "                \"image\": { \"content\": base64.b64encode(db.image_data).decode() }, \n",
    "                \"features\": [\n",
    "                    {\"type\": \"LABEL_DETECTION\", \"maxResults\": 3},\n",
    "                    {\"type\": \"FACE_DETECTION\", \"maxResults\": 3},\n",
    "                    {\"type\": \"OBJECT_LOCALIZATION\", \"maxResults\": 3},\n",
    "                    # {\"type\": \"TEXT_DETECTION\", \"maxResults\": 3, \"model\": \"builtin/latest\"},\n",
    "                    {\"type\": \"TYPE_UNSPECIFIED\", \"maxResults\": 4},\n",
    "                    {\"type\": \"LANDMARK_DETECTION\", \"maxResults\": 3},\n",
    "                    {\"type\": \"LOGO_DETECTION\", \"maxResults\": 3},\n",
    "                    {\"type\": \"SAFE_SEARCH_DETECTION\", \"maxResults\": 3},\n",
    "                    # {\"type\": \"IMAGE_PROPERTIES\", \"maxResults\": 3},\n",
    "                    # {\"type\": \"CROP_HINTS\", \"maxResults\": 3},\n",
    "                    # {\"type\": \"WEB_DETECTION\", \"maxResults\": 3},\n",
    "                    # {\"type\": \"PRODUCT_SEARCH\", \"maxResults\": 3},\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    with requests.post(db.creds[\"vision_annotate_url\"], params={\"key\": db.creds[\"vision_api_key\"]}, json=body) as r:\n",
    "        responses = r.json()\n",
    "        for response in responses['responses']:\n",
    "            for annotation in response['faceAnnotations']:\n",
    "                del annotation['landmarks']\n",
    "        pprint.pprint(responses, compact=True, width=120)\n",
    "ohmy()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Google Cloud Vision Labels:\")\n",
    "response = db.vision_client.label_detection(image=db.vision_image)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=== Google Cloud Vision Safe search:')\n",
    "response = db.vision_client.safe_search_detection(image=db.vision_image)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=== Google Cloud Vision Landmarks:')\n",
    "response = db.vision_client.landmark_detection(image=db.vision_image)\n",
    "print(response)\n",
    "landmarks = response.landmark_annotations\n",
    "\n",
    "for landmark in landmarks:\n",
    "    print(landmark.description)\n",
    "    for location in landmark.locations:\n",
    "        lat_lng = location.lat_lng\n",
    "        print('Latitude {}'.format(lat_lng.latitude))\n",
    "        print('Longitude {}'.format(lat_lng.longitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=== Google Cloud Vision Face Detection:')\n",
    "response = db.vision_client.face_detection(image=db.vision_image)\n",
    "\n",
    "pillow_image = Image.open(io.BytesIO(db.image_data)).convert(\"RGBA\")\n",
    "image_draw = ImageDraw.Draw(pillow_image)\n",
    "for i, face in enumerate(response.face_annotations):\n",
    "    vertices = (['({},{})'.format(vertex.x, vertex.y) for vertex in face.bounding_poly.vertices]) \n",
    "    print(f'Face #{i}, bounds: {\", \".join(vertices)}')\n",
    "    print(face)\n",
    "    xs = list(map(lambda v: v.x, face.bounding_poly.vertices))\n",
    "    ys = list(map(lambda v: v.y, face.bounding_poly.vertices))\n",
    "    image_draw.rectangle((min(xs), min(ys), max(xs), max(ys)), outline=colors[i%len(colors)], width=3)\n",
    "        \n",
    "from IPython import display\n",
    "pillow_image.thumbnail((512, 512))\n",
    "display.display(pillow_image)\n",
    "\n",
    "# for mark in face.landmarks:\n",
    "#     print(f'{mark.type_}({mark.position.x},{mark.position.y},{mark.position.z})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Azure Cognitive Services - Face API: Detect\")\n",
    "\n",
    "params = {\n",
    "    # Request parameters\n",
    "    'returnFaceId': 'true',\n",
    "    'returnFaceLandmarks': 'true',\n",
    "    'returnFaceAttributes': 'age,smile,facialHair,glasses,emotion,hair,makeup,accessories',\n",
    "    'recognitionModel': 'recognition_04',\n",
    "    'returnRecognitionModel': 'true',\n",
    "    'detectionModel': 'detection_01',\n",
    "    'faceIdTimeToLive': '86400',\n",
    "}\n",
    "\n",
    "faces = get_cognitive_data(\"/face/v1.0/detect\", params, db.image_data)\n",
    "# faceLandmarks pollutes the output of pprint, let's drep em'\n",
    "# for face in faces:\n",
    "#     del face['faceLandmarks']\n",
    "# pprint.pprint(faces)\n",
    "\n",
    "pillow_image = Image.open(io.BytesIO(db.image_data)).convert(\"RGBA\")\n",
    "image_draw = ImageDraw.Draw(pillow_image)\n",
    "\n",
    "for i, face in enumerate(faces):\n",
    "    face_rectangle = face[\"faceRectangle\"]\n",
    "    text = f\"\\nFace {i}\\n\\tLocation: {face_rectangle}\\n\\tAge: {face['faceAttributes']['age']}\\n\\tEmotions: {face['faceAttributes']['emotion']}\"\n",
    "    print(text)\n",
    "    image_draw.rectangle((face_rectangle['left'], face_rectangle['top'], \n",
    "            face_rectangle['left']+face_rectangle['width'], face_rectangle['top']+face_rectangle['height']), outline=colors[i%len(colors)], width=3)\n",
    "    # image_draw.multiline_text((face_rectangle['left'], face_rectangle['top']), text, font=ImageFont.truetype(\"tahoma.ttf\", size=48))\n",
    "\n",
    "from IPython import display\n",
    "pillow_image.thumbnail((512, 512))\n",
    "display.display(pillow_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Azure Cognitive Services - Vision API: Describe\")\n",
    "\n",
    "params = {\n",
    "    'maxCandidates': '3',\n",
    "    'language': 'en',\n",
    "    'model-version': 'latest',\n",
    "}\n",
    "\n",
    "data = get_cognitive_data(\"/vision/v3.2/describe\", params, db.image_data)\n",
    "pprint.pprint(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Azsure Cognitive Services - Vision API: Analyze\")\n",
    "\n",
    "params = {\n",
    "    \"visualFeatures\": \"Categories,Adult,Tags,Brands,Color,Description,Faces,ImageType,Objects\",\n",
    "    \"details\": \"Celebrities,Landmarks\",\n",
    "    \"language\": \"en\",\n",
    "    \"model-version\": \"latest\"\n",
    "}\n",
    "\n",
    "data = get_cognitive_data(\"/vision/v3.2/analyze\", params, db.image_data)\n",
    "pprint.pprint(data)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9dc8ab893de1eeb12238bd5f8a3522e0060b35d78747c82dd5784a806ff3da8c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('venvw': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
